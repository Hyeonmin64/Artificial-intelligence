### Neural Tangent Kernel: Convergence and Generalization in Neural Networks

<https://arxiv.org/abs/1806.07572>

- null

### Wide Neural Networks of Any Depth Evolve as Linear Models Under Gradient Descent

<https://arxiv.org/abs/1902.06720>

- null

### Gradient Descent Provably Optimizes Over-parameterized Neural Networks

<https://arxiv.org/abs/1810.02054>

- null

### On Exact Computation with an Infinitely Wide Neural Net

<https://arxiv.org/abs/1904.11955>

- null

### Neural Tangent Kernel Analysis of Deep Narrow Neural Networks

<https://arxiv.org/abs/2202.02981>

- null

### Neural Tangent Kernel: A Survey

<https://arxiv.org/abs/2208.13614>

- null
- With Constant kernel assumption(: For GD-dynamics, if kernel does not evolve with time),
we can get the exact solution. 
And under certain parameterization, the empirical NTK of a neural network becomes
constant as width goes to infinity. 

### Fast Finite Width Neural Tangent Kernel

<https://arxiv.org/abs/2206.08720>

-
-


### Introductory video

(Lecture 7 - Deep Learning Foundations: Neural Tangent Kernels; Soheil Feizi)

<https://www.youtube.com/watch?v=DObobAnELkU>

