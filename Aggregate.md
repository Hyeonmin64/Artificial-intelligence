### Implicit Bias of Gradient Descent on Reparametrized Models: On Equivalence to Mirror Descent

<https://arxiv.org/abs/2207.04036>

- With proper parameterization; (commuting and regular) Gradient Flow becomes equivalent with Mirror Flow.
- Also conditions to being good; (meets above) parameterization. 
- Knowledge: about "Manifold", 


### Learning dynamics of gradient descent optimization in deep neural networks

<https://link.springer.com/article/10.1007/s11432-020-3163-0>

- Dynamics(optimizer) can be viewed as control system. (ex. Thm.2 says that the SGD momentum optimizer is a second-order control system)
- Here, we deal with three optimizer and transit to different three control system.
-
- 


### Probabilistic Riemannian submanifold learning with wrapped Gaussian process latent variable models

<https://arxiv.org/abs/1805.09122>

-
-


### Attention is all you need

<https://arxiv.org/abs/1706.03762>

-
-


### A Convergence Analysis of Gradient Descent for Deep Linear Neural Networks

<https://arxiv.org/abs/1810.02281>

-
-


### Deep Learning with Functional Inputs

<https://arxiv.org/abs/2006.09590>

- Deep Neural Net is composed with both function inputs and scalar inputs.
- 


### Deep Polynomial Neural Networks

<https://arxiv.org/abs/2006.13026>

-
-


### An Interpretation of Long Short-Term Memory Recurrent Neural Network for Approximating Roots of Polynomials

<https://ieeexplore.ieee.org/document/9729726>

-
-


### The jamming transition as a paradigm to understand the loss landscape of deep neural networks

<https://arxiv.org/abs/1809.09349>

-
-


## Layer Normalization

<https://arxiv.org/abs/1607.06450>

-
-


## A scale-dependent notion of effective dimension

<https://arxiv.org/abs/2001.10872>

-
-


## Riemannian approach to batch normalization

<https://arxiv.org/abs/1709.09603>

-
-



### [Text] Kernel and Submanifold Learning

<https://link.springer.com/chapter/10.1007/978-3-642-38652-7_7>

- null


### [Text] Geometric deep learning

<https://geometricdeeplearning.com/>

- null
