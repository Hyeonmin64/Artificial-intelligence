### Implicit Bias of Gradient Descent on Reparametrized Models: On Equivalence to Mirror Descent

<https://arxiv.org/abs/2207.04036>

- With proper parameterization; (commuting and regular) Gradient Flow becomes equivalent with Mirror Flow.
- Also conditions to being good; (meets above) parameterization. 
- Knowledge: about "Manifold", 


### Learning dynamics of gradient descent optimization in deep neural networks

<https://link.springer.com/article/10.1007/s11432-020-3163-0>

- Dynamics(optimizer) can be viewed as control system. (ex. Thm.2 says that the SGD momentum optimizer is a second-order control system)
- Here, we deal with three optimizer and transit to different three control system.
-
- 


### Probabilistic Riemannian submanifold learning with wrapped Gaussian process latent variable models

<https://arxiv.org/abs/1805.09122>

-
-


### Attention is all you need

<https://arxiv.org/abs/1706.03762>

-
-


### A Convergence Analysis of Gradient Descent for Deep Linear Neural Networks

<https://arxiv.org/abs/1810.02281>

-
-





### [Text] Kernel and Submanifold Learning

<https://link.springer.com/chapter/10.1007/978-3-642-38652-7_7>

- null


### [Text] Geometric deep learning

<https://geometricdeeplearning.com/>

- null
